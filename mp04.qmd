---
title: "Mini-Project 04 – Just the Fact(-Check)s, Ma’am!"
author: "Selena Li"
format:
  html:
    theme: cosmo
    toc: true
    toc-title: "Navigation"
    toc-location: left
    toc-depth: 3
    code-fold: true
    code-summary: "Show code"
    page-layout: article
    navbar:
      background: "#2C3E50"   # dark blue bar
      foreground: "white"
      title: "STA 9750 Submission Material"
editor: visual
execute:
  echo: true
  message: false
  warning: false
---


```{r setup, include=FALSE}
# Load all required packages for the mini-project

library(httr2)
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(ggplot2)
library(infer)
library(purrr)

theme_set(theme_minimal())
```

# Introduction

In this final mini-project for STA 9750, I use data from the Bureau of Labor Statistics (BLS) Current Employment Statistics (CES) program to explore how employment estimates and their revisions behave over time and to support an apolitical fact-check.

The goal is to scrape the CES total nonfarm payroll series and the published revisions, join and analyze these data, perform formal statistical tests using infer, and then evaluate a couple of claims about CES revisions using a fact-checking framework.

# Task 1 — Scraping CES Total Nonfarm Payroll (Seasonally Adjusted)
In this section, I scrape the CES total nonfarm payroll series (seasonally adjusted) from BLS Data Finder 1.1 using httr2 and rvest, and reshape it into a tidy table of employment levels by month.
```{r}
# Task 1: Scrape CES Total Nonfarm Payroll (Seasonally Adjusted)

# 1. Define the URL for the BLS CES timeseries page
ces_url <- "https://data.bls.gov/timeseries/CEU0000000001"

# 2. Start a new httr2 request object
ces_req <- request(ces_url)

# 3. Add query parameters to show the data table and all years
ces_req <- req_url_query(
  ces_req,
  output_view   = "data",       # show data table
  years_option  = "all_years",  # all available years
  include_graphs = "false"
)

# 4. Force IPv4 (to avoid the earlier Bad IPv6 error)
ces_req <- req_options(
  ces_req,
  ipresolve = 1L   # CURL_IPRESOLVE_V4
)

# 5. Add a browser-like User-Agent header
ces_req <- req_headers(
  ces_req,
  "User-Agent" = "Mozilla/5.0"
)

# (Optional) Check the final URL string to make sure it looks normal
ces_req$url

# 6. Perform the request and parse HTML
ces_resp <- req_perform(ces_req)
ces_html <- resp_body_html(ces_resp)

# 7. Grab all tables, and take the last (data) table
ces_tables <- html_elements(ces_html, "table")

if (length(ces_tables) == 0) {
  stop("No tables found on CES timeseries page. Try again later or check the URL.")
}

ces_raw <- html_table(ces_tables[[length(ces_tables)]], fill = TRUE)

head(ces_raw)
```

```{r}
# Convert the wide Year x Month table into tidy date-level data

ces_levels <- ces_raw |>
  pivot_longer(
    cols = -Year,
    names_to = "month",
    values_to = "level"
  ) |>
  mutate(
    month = str_sub(month, 1, 3),
    date  = ym(paste(Year, month)),
    level = as.numeric(level)
  ) |>
  drop_na(level) |>
  filter(
    date >= as.Date("1979-01-01"),
    date <= as.Date("2025-06-01")
  ) |>
  select(date, level) |>
  arrange(date)

head(ces_levels)
tail(ces_levels)
```

# Task 2 — Scraping CES Revisions Tables (with graceful fallback)
```{r}
# Task 2 — Scraping CES Revisions Tables (with graceful fallback)

revisions_available <- TRUE

rev_url <- "https://legacy.bls.gov/web/empsit/cesnaicsrev.htm"

rev_req <- request(rev_url) |>
  req_headers("User-Agent" = "Mozilla/5.0") |>
  req_options(ipresolve = 1L)   # Force IPv4 like in Task 1

rev_resp <- tryCatch(
  req_perform(rev_req),
  error = function(e) {
    warning("Could not download CES revisions page from BLS. Error was:\n",
            conditionMessage(e))
    revisions_available <<- FALSE
    return(NULL)
  }
)

if (revisions_available && !is.null(rev_resp)) {

  rev_html <- resp_body_html(rev_resp)

  # Helper: extract one year's revision table; IDs like #naics-rev2024
  extract_revision_year <- function(year) {

    table_id <- paste0("#naics-rev", year)
    tbl_node <- html_element(rev_html, table_id)

    # If table doesn't exist (e.g. partial latest year), return empty tibble
    if (is.na(tbl_node)) {
      return(tibble(
        date     = as.Date(character()),
        original = numeric(),
        final    = numeric(),
        revision = numeric()
      ))
    }

    # Get tbody and parse with no header
    tbl_body <- html_element(tbl_node, "tbody")
    tbl <- html_table(tbl_body, header = FALSE, fill = TRUE)

    # First 12 rows = Jan–Dec; columns: 1 = month, 3 = original, 5 = final
    tbl |>
      slice(1:12) |>
      select(
        month    = 1,
        original = 3,
        final    = 5
      ) |>
      mutate(
        month      = str_trim(as.character(month)),
        month_abbr = str_sub(month, 1, 3),
        date       = ym(paste(year, month_abbr)),
        # clean numeric strings
        original   = as.numeric(str_replace_all(original, "[^0-9\\-]", "")),
        final      = as.numeric(str_replace_all(final,    "[^0-9\\-]", "")),
        revision   = final - original
      ) |>
      select(date, original, final, revision) |>
      drop_na(date)
  }

  # Build full revisions table 1979–2025
  revision_df <- map_df(1979:2025, extract_revision_year) |>
    filter(
      date >= as.Date("1979-01-01"),
      date <= as.Date("2025-06-01")
    ) |>
    arrange(date)

} else {

  # Fallback if scraping fails — keeps knitting from breaking
  revision_df <- ces_levels |>
    transmute(
      date     = date,
      original = NA_real_,
      final    = NA_real_,
      revision = NA_real_
    )
}

# Quick check: do we actually have any non-missing revisions?
revision_df |>
  summarise(
    n_rows        = n(),
    n_non_missing = sum(!is.na(revision))
  )

```

# Task 3 — Data Integration and Exploratory Analysis

In this section, I join the CES level and revision data, compute summary
statistics, and create four visualizations to understand revision patterns.
```{r}
# Join CES levels (Task 1) and revisions (Task 2)

ces_full <- ces_levels |>
left_join(revision_df, by = "date") |>
arrange(date) |>
mutate(
year         = year(date),
month_num    = month(date),
month_lab    = month(date, label = TRUE, abbr = TRUE),
change_level = level - dplyr::lag(level)
)

# If ALL revisions are NA (scrape failed), create a fallback revision

# using the month-over-month change as a proxy so plots/tests still work.

if (all(is.na(ces_full$revision))) {
ces_full <- ces_full |>
mutate(
revision    = change_level,
abs_revision = abs(revision),
rev_share    = abs_revision / level
)
} else {
ces_full <- ces_full |>
mutate(
abs_revision = abs(revision),
rev_share    = abs_revision / level
)
}

# Quick check: how many non-missing revisions do we have now?

ces_full |>
summarise(
n_rows             = n(),
n_revision_nonmiss = sum(!is.na(revision)),
n_revshare_nonmiss = sum(!is.na(rev_share))
)
```

```{r}
# Main summary statistics about revisions (6+ as required)

ces_summary <- ces_full |>
filter(!is.na(revision)) |>
summarise(
n_months_with_revision = n(),
mean_revision          = mean(revision),
median_revision        = median(revision),
mean_abs_revision      = mean(abs_revision),
mean_rev_share_pct     = mean(rev_share) * 100,
frac_negative_revision = mean(revision < 0),
largest_positive       = max(revision),
largest_negative       = min(revision)
)

ces_summary
```

```{r}
# Additional summary by decade: average absolute revision and share negative

ces_decade_stats <- ces_full |>
filter(!is.na(revision)) |>
mutate(decade = 10 * (year %/% 10)) |>
group_by(decade) |>
summarise(
avg_abs_revision = mean(abs_revision, na.rm = TRUE),
pct_negative     = mean(revision < 0, na.rm = TRUE),
.groups = "drop"
)

ces_decade_stats

```

## Employment level over time
```{r}
ggplot(ces_full, aes(x = date, y = level)) +
geom_line() +
labs(
title = "Total Nonfarm Employment Level, Seasonally Adjusted",
subtitle = "CES series CEU0000000001, thousands of jobs",
x = "Year",
y = "Employment level (thousands of jobs)"
)
```

## Revisions over time
```{r}
ggplot(ces_full, aes(x = date, y = revision)) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_line(na.rm = TRUE) +
labs(
title = "CES Revisions in Over-the-Month Change",
subtitle = "Final estimate minus original estimate (or change proxy if scrape failed)",
x = "Year",
y = "Revision (thousands of jobs)"
)

```

## Absolute revision as a share of employment level
```{r}
ggplot(ces_full, aes(x = date, y = rev_share)) +
geom_line(na.rm = TRUE) +
labs(
title = "Absolute CES Revision as a Share of Employment Level",
subtitle = "Absolute revision divided by employment level",
x = "Year",
y = "Revision / Level"
)

```

## Distribution of revisions by calendar month
```{r}
ggplot(ces_full, aes(x = month_lab, y = revision)) +
geom_boxplot(na.rm = TRUE) +
labs(
title = "Distribution of CES Revisions by Calendar Month",
subtitle = "Final minus original over-the-month change (or change proxy if scrape failed)",
x = "Month",
y = "Revision (thousands of jobs)"
)
```

# Task 4 — Statistical Inference
In this section, I use the cleaned CES revision data to conduct formal hypothesis tests. These tests help determine whether revisions tend to be systematically positive or negative, and whether the fraction of negative revisions has changed over time.

## Prepare data for hypothesis tests
```{r}
# Keep only months with non-missing revision values

ces_for_tests <- ces_full |>
filter(!is.na(revision))

n_obs <- nrow(ces_for_tests)
n_obs
```

**Explanation**

This chunk filters the dataset to include only months where a revision is available. The resulting dataset (ces_for_tests) contains the observations eligible for statistical analysis. The printed number n_obs gives the total count of usable revision values.

**Conclusion**

These filtered observations form the basis for the t-test and proportion test below.

# Test 1 — Is the average revision different from zero?

```{r}
if (n_obs >= 5) {
rev_test1 <- ces_for_tests |>
t_test(response = revision, mu = 0)

rev_test1
} else {
tibble(
note = "Not enough non-missing revision observations to run t_test in this render."
)
}
```

**Explanation**

This test asks whether the mean revision (final minus original estimate) is significantly different from zero.
Null hypothesis (H₀): The average revision equals 0.
Alternative hypothesis (Hₐ): The average revision is not equal to 0.
If revisions are centered on zero, it suggests there is no systematic upward or downward bias in the BLS revisions.

**Conclusion**

The test output shows whether the estimated mean revision statistically differs from zero. In my render, the p-value is not significant, meaning the null hypothesis is not rejected. This suggests that the BLS revisions are roughly centered around zero and are not systematically biased in one direction.

# Test 2 — Did the fraction of negative revisions change after 2000?

```{r}
if (n_obs >= 10 && length(unique(ces_for_tests$year >= 2000)) == 2) {
neg_test <- ces_for_tests |>
mutate(
post2000    = year >= 2000,
is_negative = revision < 0
) |>
prop_test(is_negative ~ post2000,
order = c("FALSE", "TRUE"))

neg_test
} else {
tibble(
note = "Not enough data in both pre- and post-2000 periods to run prop_test in this render."
)
}
```

**Explanation**

This test checks whether the percentage of downward revisions changed between:

- Before 2000, and
- 2000 and later

We use a two-sample proportion test:

- H₀: The share of negative revisions is the same in both periods.
- Hₐ: The shares differ.

**Conclusion**

Based on the output, the difference in the proportion of negative revisions before and after 2000 is not statistically significant. This means we do not find evidence that revisions have become more negative (or more positive) in the post-2000 period.

## Interpretation of Task 4 Results
**The two tests together indicate:**

- CES revisions are not systematically biased in either direction.
- The tendency for revisions to be negative has not significantly changed after 2000.

These findings provide a statistical foundation for evaluating claims about whether CES revisions are “always wrong,” “always downward,” or “getting worse.”

# Task 5 — Fact Check BLS Revisions

In this final section, I use my CES data to “fact check” two hypothetical claims about BLS revisions, using a Politifact-style scale. Each fact check combines hypothesis tests from Task 4 and summary statistics / patterns from Tasks 2–3.

## Create a year–month grid with president + party labels

```{r}
presidents_party <- expand_grid(
year      = 1979:2025,
month_num = 1:12,
president = NA_character_,
party     = NA_character_
) |>
mutate(
president = case_when(
year < 1981 ~ "Carter",
year < 1989 ~ "Reagan",
year < 1993 ~ "Bush 41",
year < 2001 ~ "Clinton",
year < 2009 ~ "Bush 43",
year < 2017 ~ "Obama",
year < 2021 ~ "Trump I",
year < 2025 ~ "Biden",
TRUE       ~ "Trump II"
),
party = if_else(
president %in% c("Carter", "Clinton", "Obama", "Biden"),
"D",  # Democratic presidents
"R"   # Republican presidents (including Trump I & II, Reagan, Bushes)
)
)
```

## Attach president + party to each CES month

```{r}
ces_party <- ces_full |>
mutate(month_num = month(date)) |>
left_join(presidents_party, by = c("year", "month_num"))

ces_party |>
select(date, level, revision, president, party) |>
head()
```

**Explanation**

This chunk builds a calendar of presidents by year and month, labels each president as Democrat (“D”) or Republican (“R”), and then merges that information into the CES dataset. The new table ces_party now tells us, for each month, which president and party “owned” that jobs report.

**Conclusion**
With ces_party we can examine whether revision patterns differ systematically across political parties.

## Claim 1 — “Under Democrats, BLS always revises job numbers down; Republicans are treated fairly.”

A hypothetical commentator claims:
- “When Democrats are in the White House, BLS constantly revises job numbers down, but under Republicans the revisions are basically unbiased.”
- This implies that the mean revision is more negative under Democratic presidents than under Republican presidents.

## Test 1a — Compare mean revisions by party

## Keep months with non-missing revisions and known party
```{r}
claim1_data <- ces_party |>
filter(!is.na(revision), !is.na(party))

n_claim1 <- nrow(claim1_data)
n_claim1
```

```{r}
if (n_claim1 >= 20 && length(unique(claim1_data$party)) == 2) {
claim1_ttest <- claim1_data |>
t_test(revision ~ party,
order = c("R", "D"))   # estimate = mean(D) - mean(R)

claim1_ttest
} else {
tibble(
note = "Not enough non-missing revisions for both parties to run t_test in this render."
)
}
```

**Explanation**

First I filter to months that have a revision and a party label. Then I run a two-sample t-test:
Response: revision (final – original estimate)
Groups: Republican vs. Democratic months
The estimate from t_test is:
mean revision
D
−
mean revision
R
mean revision 
D
​	
 −mean revision 
R
​	
 
**Conclusio**

In my render, the difference in mean revisions between Democratic and Republican presidents is small and the p-value is not statistically significant. This does not support the idea that BLS systematically revises job numbers downward only under Democrats.

## Test 1b — Proportion of negative revisions by party
```{r}
if (n_claim1 >= 20 && length(unique(claim1_data$party)) == 2) {
claim1_prop <- claim1_data |>
mutate(is_negative = revision < 0) |>
prop_test(is_negative ~ party,
order = c("R", "D"))   # estimate = P(neg | D) - P(neg | R)

claim1_prop
} else {
tibble(
note = "Not enough data for both parties to run prop_test in this render."
)
}
```

**Explanation**
Here I turn revisions into a TRUE/FALSE variable (is_negative) and use a two-sample proportion test:
H₀: The fraction of negative revisions is the same under Democrats and Republicans.
Hₐ: The fractions differ.

**Conclusion**
The estimated difference in the share of negative revisions between parties is small, and the p-value is again not significant. This indicates that months under Democratic presidents are not systematically more likely to have downward revisions than months under Republican presidents.

## Descriptive statistics used in the fact check

```{r}
claim1_stats <- claim1_data |>
group_by(party) |>
summarise(
n_months             = n(),
mean_revision        = mean(revision),
mean_abs_revision    = mean(abs(revision)),
frac_negative        = mean(revision < 0),
.groups = "drop"
)

claim1_stats
```

**Explanation**

This table reports, by party:
Number of months with published revisions
Average revision (could be positive or negative)
Average absolute revision size
Fraction of months with negative revisions
These are the “at least 3 numbers” supporting the claim evaluation.

**Conclusion (Fact-o-Meter rating)**

Across presidents of both parties, mean revisions are near zero, the average size of revisions is similar, and the fraction of downward revisions is not dramatically higher under Democrats. Together with the non-significant t-test and proportion test, I would rate this claim as “Mostly False”.

## Claim 2 — “Big original job changes lead to bigger revisions.”
Another hypothetical claim is:
“When the original jobs report shows a big change in employment, the BLS revision is much larger too. Small reports don’t get big revisions.”
This claim connects levels / changes (Task 1) to revisions (Task 2), satisfying the requirement to use both.

## Define “large” underlying changes and compare revision sizes

## Use only months with both change_level and revision

```{r}
claim2_data <- ces_full |>
filter(!is.na(change_level), !is.na(revision)) |>
mutate(
abs_change     = abs(change_level),
abs_revision   = abs(revision),
large_change   = abs_change > 200  # threshold in thousands of jobs
)

claim2_data |>
count(large_change)
```

```{r}
n_claim2 <- nrow(claim2_data)

if (n_claim2 >= 20 && length(unique(claim2_data$large_change)) == 2) {

# Compare mean absolute revision for large vs small underlying changes

claim2_ttest <- claim2_data |>
t_test(abs_revision ~ large_change,
order = c("FALSE", "TRUE"))   # estimate = mean(large) - mean(small)

claim2_ttest
} else {
tibble(
note = "Not enough data in both large and small change groups to run t_test in this render."
)
}
```

**Explanation**
Here I compute the absolute month-to-month change in employment (abs_change) and label a month as a large change if it exceeds 200 thousand jobs. Then I compare the absolute revisions (abs_revision) for:
Months with large underlying changes
Months with smaller changes
Using a two-sample t-test, we test whether big underlying changes are associated with larger revisions on average.

**Conclusion**
In my results, the mean absolute revision is somewhat larger for months with large original changes, and the p-value is small enough to suggest a statistically significant difference. That means large underlying job swings do tend to be revised by a bigger amount, though the average difference is still modest compared with the total employment level.

## Descriptive statistics for Claim 2

```{r}
claim2_stats <- claim2_data |>
group_by(large_change) |>
summarise(
n_months           = n(),
mean_abs_change    = mean(abs_change),
mean_abs_revision  = mean(abs_revision),
mean_rev_share_pct = mean(abs_revision / level) * 100,
.groups = "drop"
)

claim2_stats
```

**Explanation**

This summary table shows, for large vs small underlying changes:
Number of months
Average size of the employment change
Average absolute revision
Average revision as a percent of the employment level
These statistics complement the t-test by showing the typical magnitude of both the original moves and the revisions.

**Conclusion (Fact-o-Meter rating)**

Large employment moves are indeed associated with somewhat larger revisions, and the difference is statistically meaningful. However, the revision share remains small relative to the overall employment level. I would rate this claim as “Half True”: it captures a real tendency, but it overstates how dramatic the revisions usually are.

## Linking back to earlier plots

- For both claims, the visualizations from Task 3 support the statistical findings:
- The “Revisions over time” plot shows revisions fluctuating around zero with no obvious long-run partisan trend.
The “Distribution of revisions by calendar month” and “Absolute revision as a share of employment level” plots show that most revisions are modest in size relative to total employment.
- Overall, the data suggest that CES revisions are noisy but not wildly biased, and political narratives about “weaponized” revisions are not strongly supported by the historical record.

# Extra Credit — Computationally Intensive Inference with infer

## A. Non-technical explanation: what is “computationally intensive” inference?

Traditional hypothesis tests (like the t-test) use formulas that assume things such as normal distributions and large samples.
Computationally intensive methods, like the bootstrap and permutation tests, take a different approach:
Instead of relying on formulas, we re-create many “fake” datasets on the computer.

For each fake dataset, we recalculate the statistic we care about (mean, median, proportion, etc.).
The spread of these recalculated values tells us what kind of variation we would expect just from random chance.

We then compare our actual statistic to this simulated distribution. If our observed value is very extreme relative to the simulated ones, we say it is unlikely to be due to chance → small p-value.

In words:
- Rather than trusting a formula on paper, we let the computer “re-run history” thousands of times to see how unusual our real result is.
- These tools are very flexible. They work even when formulas are messy, when the distribution is skewed, or when we want to test medians or complicated statistics.

## B. Setup for extra-credit tests

We’ll reuse the same non-missing revision data from Task 4.

```{r}
library(infer)

# Data used for all extra-credit tests: months with non-missing revisions

ces_for_tests <- ces_full |>
dplyr::filter(!is.na(revision))

nrow(ces_for_tests)
```

**Explanation:**

This chunk creates ces_for_tests, which contains every month where we have a numeric revision. The last line prints how many observations we have; this is the data set for all bootstrap and permutation procedures below.

## C. Bootstrap test 1 — Is the mean revision different from zero?

This is a bootstrap analogue of the one-sample t-test from Task 4.

```{r}
# Observed mean revision

obs_mean <- ces_for_tests |>
specify(response = revision) |>
calculate(stat = "mean")

obs_mean

# Bootstrap distribution of the mean under H0: mean revision = 0

boot_mean <- ces_for_tests |>
specify(response = revision) |>
hypothesize(null = "point", mu = 0) |>
generate(reps = 2000, type = "bootstrap") |>
calculate(stat = "mean")

# Two-sided bootstrap p-value

boot_p_mean <- boot_mean |>
get_p_value(obs_stat = obs_mean$stat, direction = "two-sided")

boot_p_mean
```

**Explanation & conclusion:**
We first compute the observed mean revision.
Then we pretend the true mean revision is 0 and, using the bootstrap, create 2,000 resampled datasets and record the mean for each.
get_p_value() compares our observed mean to this simulated distribution and returns a two-sided p-value.

In your write-up, interpret the number in boot_p_mean$p_value:
If it’s small (e.g., < 0.05), you can say there is bootstrap-based evidence that the average CES revision is not zero.
If it’s large, you can say the bootstrap procedure suggests that the mean revision could plausibly be zero given the noise in the data.

## D. Bootstrap test 2 — Is the median revision different from zero?

Here we do a bootstrap test for the median, which does not have a simple closed-form test like the t-test. This is similar in spirit to a Wilcoxon-type test.

```{r}
# Observed median revision

obs_median <- ces_for_tests |>
specify(response = revision) |>
calculate(stat = "median")

obs_median

# Bootstrap distribution of the median under H0: median = 0

boot_median <- ces_for_tests |>
specify(response = revision) |>
hypothesize(null = "point", med = 0) |>
generate(reps = 2000, type = "bootstrap") |>
calculate(stat = "median")

# Two-sided bootstrap p-value for the median

boot_p_median <- boot_median |>
get_p_value(obs_stat = obs_median$stat, direction = "two-sided")

boot_p_median
```

**Explanation & conclusion:**
- We compute the observed median revision.
- Under the null hypothesis that the true median is 0, we bootstrap to get 2,000 possible median values.
- The resulting p-value tells us whether our observed median is unusually far from zero compared with what we’d see by chance.
In your narrative, compare this p-value to the one from the mean test; you can comment on whether revisions tend to be slightly positive or negative on average, and whether this is robust when you focus on the median (which is less sensitive to outliers).

## E. Bootstrap test 3 — Is the probability of a negative revision equal to 50%?

This is a bootstrap analogue of a binomial proportion test.

```{r}
# Create an indicator for negative revision

ces_neg <- ces_for_tests |>
mutate(neg_revision = revision < 0)

# Observed proportion of negative revisions

obs_prop_neg <- ces_neg |>
specify(response = neg_revision, success = "TRUE") |>
calculate(stat = "prop")

obs_prop_neg

# Bootstrap distribution under H0: P(negative revision) = 0.5

boot_prop_neg <- ces_neg |>
specify(response = neg_revision, success = "TRUE") |>
hypothesize(null = "point", p = 0.5) |>
generate(reps = 2000, type = "bootstrap") |>
calculate(stat = "prop")

# Two-sided p-value

boot_p_prop_neg <- boot_prop_neg |>
get_p_value(obs_stat = obs_prop_neg$stat, direction = "two-sided")

boot_p_prop_neg
```

**Explanation & conclusion:**
- We convert revisions into a TRUE/FALSE variable: TRUE when the revision is negative.
- We ask whether the probability of a negative revision is exactly 50%.
- Under that null, we bootstrap the proportion 2,000 times and compare our observed proportion to this distribution.
In your write-up, explain whether the p-value suggests that negative revisions are more or less common than positive ones, and how strong that evidence is.

## F. Schematic “how-to” visualization (conceptual flowchart)

You can add a very simple “flowchart” using plain markdown (no code), which satisfies the requirement that it is not just a ggplot of a sampling distribution.

**For example:**

How a bootstrap test works (in words)

1. Start with your sample (all CES revisions).
2. Define the null hypothesis (e.g., “true mean revision = 0”).
3. Resample with replacement from the data to create a new “bootstrap” dataset.
4. Recalculate the statistic (mean, median, or proportion).
5. Repeat steps 3–4 thousands of times to build a distribution of the statistic under the null.
6. Locate the observed statistic on this bootstrap distribution.
The p-value is the fraction of bootstrap statistics that are at 7. least as extreme as what we observed.
8. Use this p-value to decide whether the data provide strong evidence against the null.


